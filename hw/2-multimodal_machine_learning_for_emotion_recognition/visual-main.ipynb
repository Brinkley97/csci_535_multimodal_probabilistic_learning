{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e79fb63-a451-4698-8ef0-208e6806b122",
   "metadata": {},
   "source": [
    "# HW 2 Multimodal Machine Learning for Emotion Recognition\n",
    "\n",
    "- main with sub notebooks\n",
    "    1. audio (acoustic) \n",
    "    2. text (lexical) \n",
    "    3. visual (this notebook)\n",
    "    \n",
    "- `%load` and `%run` with [cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-load): This allows us to get code from another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db78a5cf-1064-4bed-869f-47a900287f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_notebook = 'main.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589c0619-e7d7-4942-9097-1bdd9818be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main_notebook\n",
    "main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13001ba1-b17e-4054-977e-a5af0910f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'main.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a01692-36e2-45ca-9504-932cf84ca286",
   "metadata": {},
   "source": [
    "- If video, the original video will be a 3D with a T dimension. This specific case, we have a 2D of (T, 2048) which is based on the ResNet encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d90de4-7088-46b2-a5c9-de10f93a9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_with_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff97cf8-1eea-4509-ac02-1f166a683252",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = features_with_y.loc[0:, 'emotion_labels']\n",
    "# ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a974ad6e-8199-4326-bbef-9e676b470963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_temporal_dimension(visual_features_paths, ys):\n",
    "    \"\"\"Reduce from 2D to 1D by removing the time dimension\n",
    "    \n",
    "    visual_features_paths -- list\n",
    "    \n",
    "    Return \n",
    "    reduced shapes of each input -- list\n",
    "    \"\"\"\n",
    "    reduced_visual_features = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for row in range(len(visual_features_paths)):\n",
    "        # print(ys[row])\n",
    "        # print(\"Current path with files is: \", visual_features_path)\n",
    "        path_exists = os.path.exists(visual_features_paths[row])\n",
    "        # print(path_exists)\n",
    "        if path_exists == True:\n",
    "            # print(\"Current path with files is: \", visual_features_path)\n",
    "            load_visual_features_file = np.load(visual_features_paths[row])\n",
    "            # print(\"  Original Shape: \", np.shape(load_visual_features_file))\n",
    "            resampled_visual = np.mean(load_visual_features_file, axis=0)\n",
    "            reduced_visual_features.append(resampled_visual)\n",
    "            # print(\"  Reduced shape: \", np.shape(resampled_visual))\n",
    "            # print()\n",
    "            true_labels.append(ys[row])\n",
    "        else:\n",
    "            # print()\n",
    "            \n",
    "            # print(\"\\nCANNOT find current path: \", visual_features_paths[row])\n",
    "            # print(\"DoNOT include\", ys[row])\n",
    "            pass\n",
    "    return reduced_visual_features, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad03533c-25ec-4440-954b-2c7975ee827d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1003, 2048), (1003,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_temporal_dimensions, true_labels = reduce_temporal_dimension(visual_features_paths, ys)\n",
    "np.shape(reduced_temporal_dimensions), np.shape(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022de293-812c-4c9b-855e-a9b9146b13d7",
   "metadata": {},
   "source": [
    "See my [HW 2 Notes](https://detraviousjbrinkley.notion.site/HW-2-Multimodal-Machine-Learning-for-Emotion-Recognition-4967e03039b04f20907996de25f6068a) for more details on \n",
    "1. Support Vector Machine (SVM)\n",
    "2. Support Vector Classification (SVC)\n",
    "3. Note that LinearSVC does not accept parameter kernel, as this is assumed to be linear per the [SVM documentation](https://scikit-learn.org/stable/modules/svm.html#multi-class-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9bd1175-c25c-4921-9e4f-28e15fb32972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=LinearSVC(),\n",
       "             param_grid={&#x27;SVC__C&#x27;: array([1.000e-02, 1.001e+01, 2.001e+01, 3.001e+01, 4.001e+01, 5.001e+01,\n",
       "       6.001e+01, 7.001e+01, 8.001e+01, 9.001e+01])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=LinearSVC(),\n",
       "             param_grid={&#x27;SVC__C&#x27;: array([1.000e-02, 1.001e+01, 2.001e+01, 3.001e+01, 4.001e+01, 5.001e+01,\n",
       "       6.001e+01, 7.001e+01, 8.001e+01, 9.001e+01])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10, estimator=LinearSVC(),\n",
       "             param_grid={'SVC__C': array([1.000e-02, 1.001e+01, 2.001e+01, 3.001e+01, 4.001e+01, 5.001e+01,\n",
       "       6.001e+01, 7.001e+01, 8.001e+01, 9.001e+01])})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifier = svm.LinearSVC()\n",
    "parameters = {'SVC__C':np.arange(0.01,100,10)}\n",
    "classification = GridSearchCV(linear_classifier, param_grid=parameters, cv=10)\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41f000bc-4d13-4ecf-ad51-929bee7a3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification.fit(reduced_temporal_dimensions, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfd0aa-a215-419c-adec-b7bb912836d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
