{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73285297-aaac-4a84-9674-1e34fd5954c6",
   "metadata": {},
   "source": [
    "# Extracting Features with ViT\n",
    "- https://huggingface.co/docs/transformers/model_doc/vit#vision-transformer-vit\n",
    "- https://arxiv.org/abs/2010.11929\\\n",
    "- Only a subset of features as kernel times out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec1477-5216-45f8-8bd2-a178186168bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import ViTImageProcessor, ViTFeatureExtractor, ViTModel\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fdf4ea-1350-468c-b1a4-1bcb9379d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba20bd1-54fe-4b63-af56-df2ee86d7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/Users/brinkley97/Documents/development/'\n",
    "CLASS_PATH = 'classes/csci_535_multimodal_probabilistic_learning/'\n",
    "DATASET_PATH = 'datasets/project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c914f-b121-453d-a043-b7bf0f999171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    original_data = pd.read_csv(file)\n",
    "    # original_data = pd.DataFrame(file)\n",
    "    copy_of_data = original_data.copy()\n",
    "    return copy_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f2dc55-9fb5-44ab-a50c-5bdedc865117",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = BASE + CLASS_PATH + DATASET_PATH + 'crema_d/paths_to_crema_d_emotion_folder_images.csv'\n",
    "# file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3281a3-a993-49a7-82aa-6e2aef2809ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths_copy = load_data(file_paths)\n",
    "# dataset_paths_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdea021-aaf4-4ba7-9ecf-cc018693ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(specific_dataset_frames):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    faces_in_specific_folder_path -- py str (of all faces in a specific folder)\n",
    "    faces_file_names -- py list (of all the file names in a specific folder)\n",
    "    \n",
    "    Return:\n",
    "    extracted_features -- py\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_features_per_video = list() #torch.empty((0, 197, 768))\n",
    "\n",
    "    for specific_dataset_frame_idx in range(len(specific_dataset_frames)):\n",
    "  \n",
    "        specific_frame_path = specific_dataset_frames[specific_dataset_frame_idx]\n",
    "        print(specific_dataset_frame_idx, specific_frame_path)\n",
    "        \n",
    "        specific_frame = Image.open(specific_frame_path)\n",
    "\n",
    "        inputs = processor(images=specific_frame, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # last_hidden_states == representation (1 and 2 with GradCam)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        extracted_features_per_video.append(last_hidden_states) #torch.vstack((extracted_features_per_video, last_hidden_states))\n",
    "        \n",
    "    return extracted_features_per_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41cb6c2-c390-4074-8f3a-a7389723d703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "220f9b18-5f69-4863-a454-274d2015bcb4",
   "metadata": {},
   "source": [
    "# CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759bc3c-19f4-4b3e-9705-3c92ec1ff14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ang_crema_d = list(dataset_paths_copy['ang'].dropna())\n",
    "# len(ang_crema_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5a7f0-273c-4010-a88b-661f7dbcc0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ang_crema_d[10:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ce244-b850-495d-a951-db22444e37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ang_crema_d[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff3cf3-b445-47c0-9da7-83dfe5074fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ang_vit_features_crema_d_1000 = extract_features(ang_crema_d[0:1000])\n",
    "# ang_vit_features_crema_d_2000 = extract_features(ang_crema_d[1000:2000])\n",
    "# ang_vit_features_crema_d_3000 = extract_features(ang_crema_d[2000:3000])\n",
    "# ang_vit_features_crema_d_4000 = extract_features(ang_crema_d[3000:4000])\n",
    "# ang_vit_features_crema_d_5000 = extract_features(ang_crema_d[4000:5000])\n",
    "# ang_vit_features_crema_d_6000 = extract_features(ang_crema_d[5000:6000])\n",
    "# ang_vit_features_crema_d_7000 = extract_features(ang_crema_d[6000:7000])\n",
    "# ang_vit_features_crema_d_8000 = extract_features(ang_crema_d[7000:8000])\n",
    "# ang_vit_features_crema_d_9000 = extract_features(ang_crema_d[8000:9000])\n",
    "# ang_vit_features_crema_d_10000 = extract_features(ang_crema_d[9000:10000])\n",
    "# ang_vit_features_crema_d_11000 = extract_features(ang_crema_d[10000:11000])\n",
    "\n",
    "# need to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e63c05-353d-44ca-acbc-70d49e7efcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ang_vit_features_crema_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1995ab-cae8-464e-9cfe-df9b83e55389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_extracted_features_crema_d = BASE + CLASS_PATH + DATASET_PATH + 'crema_d/extracted_features_crema_d/ang_12000_extracted_features_crema_d.pt'\n",
    "# torch.save(ang_vit_features_crema_d_12000, save_extracted_features_crema_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827458c2-4f57-4b06-aeed-1d9f59d4b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(save_extracted_features_crema_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93448141-1f83-42c3-99e4-7d9a6ce57af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_crema_d = list(dataset_paths_copy['dis'].dropna())\n",
    "# len(dis_crema_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f07cea-f678-4677-90a7-2628529f8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dis_vit_features_crema_d_1000 = extract_features(dis_crema_d[0:1000])\n",
    "# dis_vit_features_crema_d_2000 = extract_features(dis_crema_d[1000:2000])\n",
    "# dis_vit_features_crema_d_3000 = extract_features(dis_crema_d[2000:3000])\n",
    "# dis_vit_features_crema_d_4000 = extract_features(dis_crema_d[3000:4000])\n",
    "# dis_vit_features_crema_d_5000 = extract_features(dis_crema_d[4000:5000])\n",
    "# dis_vit_features_crema_d_6000 = extract_features(dis_crema_d[5000:6000])\n",
    "# dis_vit_features_crema_d_7000 = extract_features(dis_crema_d[6000:7000])\n",
    "# dis_vit_features_crema_d_8000 = extract_features(dis_crema_d[7000:8000])\n",
    "# dis_vit_features_crema_d_9000 = extract_features(dis_crema_d[8000:9000])\n",
    "\n",
    "# need to run\n",
    "# dis_vit_features_crema_d_10000 = extract_features(dis_crema_d[9000:10000])\n",
    "# dis_vit_features_crema_d_11000 = extract_features(dis_crema_d[10000:11000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad65f90-4d10-4799-928b-6b5c61ae4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_extracted_features_crema_d = BASE + CLASS_PATH + DATASET_PATH + 'crema_d/extracted_features_crema_d/dis/dis_9000_extracted_features_crema_d.pt'\n",
    "# torch.save(dis_vit_features_crema_d_9000, save_extracted_features_crema_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca1b0f5-8e40-45e8-92f2-f0cedc517d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_crema_d = list(dataset_paths_copy['fea'].dropna())\n",
    "len(fea_crema_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd2f4b4-aecb-4870-aee3-7e91746cd430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fea_vit_features_crema_d_1000 = extract_features(fea_crema_d[0:1000])\n",
    "# fea_vit_features_crema_d_2000 = extract_features(fea_crema_d[1000:2000])\n",
    "# fea_vit_features_crema_d_3000 = extract_features(fea_crema_d[2000:3000])\n",
    "# fea_vit_features_crema_d_4000 = extract_features(fea_crema_d[3000:4000])\n",
    "fea_vit_features_crema_d_5000 = extract_features(fea_crema_d[4000:5000])\n",
    "\n",
    "# need to run\n",
    "# fea_vit_features_crema_d_6000 = extract_features(fea_crema_d[5000:6000])\n",
    "# fea_vit_features_crema_d_7000 = extract_features(fea_crema_d[6000:7000])\n",
    "# fea_vit_features_crema_d_8000 = extract_features(fea_crema_d[7000:8000])\n",
    "# fea_vit_features_crema_d_9000 = extract_features(fea_crema_d[8000:9000])\n",
    "# fea_vit_features_crema_d_10000 = extract_features(fea_crema_d[9000:10000])\n",
    "# fea_vit_features_crema_d_11000 = extract_features(fea_crema_d[10000:11000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279eaff-46b3-4546-9ca3-9950554fa085",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_extracted_features_crema_d = BASE + CLASS_PATH + DATASET_PATH + 'crema_d/extracted_features_crema_d/fea/fea_5000_extracted_features_crema_d.pt'\n",
    "torch.save(fea_vit_features_crema_d_5000, save_extracted_features_crema_d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
