{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cba5bc0-6146-4d9d-b358-4f9dba73e869",
   "metadata": {},
   "source": [
    "# Loading ViT Feature Extractor Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b216be6f-0f7e-44b4-acfc-23c888b118b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset beans (/Users/brinkley97/.cache/huggingface/datasets/beans/default/0.0.0/90c755fb6db1c0ccdad02e897a37969dbf070bed3755d4391e269ff70642d791)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91345b013ae04177bdf1406cbaa87e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_file_path', 'image', 'labels'],\n",
       "        num_rows: 1034\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image_file_path', 'image', 'labels'],\n",
       "        num_rows: 133\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_file_path', 'image', 'labels'],\n",
       "        num_rows: 128\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('beans')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b138d6c8-dec8-4f32-9d58-746f03dfd779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_file_path': '/Users/brinkley97/.cache/huggingface/datasets/downloads/extracted/d91219caa561dea80e07fdc1f4941612865d20fcd0329f9fbc65a77755e5d9c8/train/bean_rust/bean_rust_train.148.jpg',\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500>,\n",
       " 'labels': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = ds['train'][400]\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c134c6-f9d0-49ba-93d4-6f112032b11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PIL.JpegImagePlugin.JpegImageFile,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = ex['image']\n",
    "type(image), image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4027ca9f-f21b-4618-a435-198d06ab4895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 22:01:32.224912: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/brinkley97/opt/anaconda3/envs/emotion_rec_from_audiovisual/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e20f2d-dea7-4ee0-bc08-bb9fcb4424f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454be1a8-cd6b-4076-be80-4f74b44d788c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': [array([[[ 0.7882353 ,  0.67058825,  0.70980394, ..., -0.19215685,\n",
       "         -0.12941176, -0.17647058],\n",
       "        [ 0.7019608 ,  0.6       ,  0.6862745 , ..., -0.2862745 ,\n",
       "         -0.1607843 , -0.1607843 ],\n",
       "        [ 0.48235297,  0.4039216 ,  0.4666667 , ..., -0.19215685,\n",
       "         -0.01960784,  0.02745104],\n",
       "        ...,\n",
       "        [ 0.3803922 ,  0.5294118 ,  0.48235297, ..., -0.827451  ,\n",
       "         -0.81960785, -0.8039216 ],\n",
       "        [ 0.09019613,  0.37254906,  0.3803922 , ..., -0.8666667 ,\n",
       "         -0.84313726, -0.8509804 ],\n",
       "        [-0.05098039,  0.27843142,  0.3176471 , ..., -0.85882354,\n",
       "         -0.827451  , -0.827451  ]],\n",
       "\n",
       "       [[ 0.4901961 ,  0.34901965,  0.3803922 , ..., -0.60784316,\n",
       "         -0.5372549 , -0.58431375],\n",
       "        [ 0.35686278,  0.20000005,  0.3176471 , ..., -0.7176471 ,\n",
       "         -0.6       , -0.5921569 ],\n",
       "        [ 0.04313731, -0.09019607,  0.05098045, ..., -0.6392157 ,\n",
       "         -0.47450978, -0.42745095],\n",
       "        ...,\n",
       "        [-0.2235294 , -0.05098039, -0.09019607, ..., -0.9764706 ,\n",
       "         -0.9607843 , -0.9372549 ],\n",
       "        [-0.5058824 , -0.20784312, -0.19215685, ..., -0.99215686,\n",
       "         -0.99215686, -1.        ],\n",
       "        [-0.64705884, -0.30196077, -0.24705881, ..., -0.9843137 ,\n",
       "         -0.9764706 , -0.9843137 ]],\n",
       "\n",
       "       [[ 0.43529415,  0.27058828,  0.30980396, ..., -0.7176471 ,\n",
       "         -0.6392157 , -0.6862745 ],\n",
       "        [ 0.23921573,  0.082353  ,  0.20784318, ..., -0.8117647 ,\n",
       "         -0.67058825, -0.654902  ],\n",
       "        [-0.12156862, -0.23921567, -0.08235294, ..., -0.7019608 ,\n",
       "         -0.5372549 , -0.4980392 ],\n",
       "        ...,\n",
       "        [-0.27843136, -0.12941176, -0.19999999, ..., -0.94509804,\n",
       "         -0.94509804, -0.94509804],\n",
       "        [-0.6       , -0.30196077, -0.31764704, ..., -0.9843137 ,\n",
       "         -0.9764706 , -0.99215686],\n",
       "        [-0.7647059 , -0.40392154, -0.38823527, ..., -0.9764706 ,\n",
       "         -0.96862745, -0.96862745]]], dtype=float32)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51027275-428a-4a6b-9d70-57ed60a13535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[ 0.7882,  0.6706,  0.7098,  ..., -0.1922, -0.1294, -0.1765],\n",
       "          [ 0.7020,  0.6000,  0.6863,  ..., -0.2863, -0.1608, -0.1608],\n",
       "          [ 0.4824,  0.4039,  0.4667,  ..., -0.1922, -0.0196,  0.0275],\n",
       "          ...,\n",
       "          [ 0.3804,  0.5294,  0.4824,  ..., -0.8275, -0.8196, -0.8039],\n",
       "          [ 0.0902,  0.3725,  0.3804,  ..., -0.8667, -0.8431, -0.8510],\n",
       "          [-0.0510,  0.2784,  0.3176,  ..., -0.8588, -0.8275, -0.8275]],\n",
       "\n",
       "         [[ 0.4902,  0.3490,  0.3804,  ..., -0.6078, -0.5373, -0.5843],\n",
       "          [ 0.3569,  0.2000,  0.3176,  ..., -0.7176, -0.6000, -0.5922],\n",
       "          [ 0.0431, -0.0902,  0.0510,  ..., -0.6392, -0.4745, -0.4275],\n",
       "          ...,\n",
       "          [-0.2235, -0.0510, -0.0902,  ..., -0.9765, -0.9608, -0.9373],\n",
       "          [-0.5059, -0.2078, -0.1922,  ..., -0.9922, -0.9922, -1.0000],\n",
       "          [-0.6471, -0.3020, -0.2471,  ..., -0.9843, -0.9765, -0.9843]],\n",
       "\n",
       "         [[ 0.4353,  0.2706,  0.3098,  ..., -0.7176, -0.6392, -0.6863],\n",
       "          [ 0.2392,  0.0824,  0.2078,  ..., -0.8118, -0.6706, -0.6549],\n",
       "          [-0.1216, -0.2392, -0.0824,  ..., -0.7020, -0.5373, -0.4980],\n",
       "          ...,\n",
       "          [-0.2784, -0.1294, -0.2000,  ..., -0.9451, -0.9451, -0.9451],\n",
       "          [-0.6000, -0.3020, -0.3176,  ..., -0.9843, -0.9765, -0.9922],\n",
       "          [-0.7647, -0.4039, -0.3882,  ..., -0.9765, -0.9686, -0.9686]]]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor(image, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d4b545-5c2a-450d-bdb8-d2a68fd112d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    inputs = feature_extractor(example['image'], return_tensors='pt')\n",
    "    inputs['labels'] = example['labels']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e8d806-2fe0-4662-a36f-fd7e3f564a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.5686, -0.5686, -0.5608,  ..., -0.0275,  0.1922, -0.2549],\n",
       "          [-0.6078, -0.6000, -0.5843,  ..., -0.0353, -0.0196, -0.2706],\n",
       "          [-0.6314, -0.6314, -0.6157,  ..., -0.2392, -0.3647, -0.2314],\n",
       "          ...,\n",
       "          [-0.5373, -0.5529, -0.5765,  ..., -0.0745, -0.0431, -0.0980],\n",
       "          [-0.5608, -0.5765, -0.5843,  ...,  0.3176,  0.1608,  0.1294],\n",
       "          [-0.5843, -0.5922, -0.6078,  ...,  0.2784,  0.1451,  0.2000]],\n",
       "\n",
       "         [[-0.7098, -0.7098, -0.7490,  ..., -0.3725, -0.1608, -0.6000],\n",
       "          [-0.7333, -0.7333, -0.7569,  ..., -0.3569, -0.3176, -0.5608],\n",
       "          [-0.7490, -0.7490, -0.7647,  ..., -0.5373, -0.6627, -0.5373],\n",
       "          ...,\n",
       "          [-0.7725, -0.7882, -0.8196,  ..., -0.2314, -0.0353,  0.0824],\n",
       "          [-0.7961, -0.8118, -0.8118,  ...,  0.1843,  0.3176,  0.3725],\n",
       "          [-0.8196, -0.8196, -0.8275,  ...,  0.0745,  0.2863,  0.3961]],\n",
       "\n",
       "         [[-0.9922, -0.9922, -1.0000,  ..., -0.5451, -0.3647, -0.7333],\n",
       "          [-0.9922, -0.9922, -1.0000,  ..., -0.5686, -0.5451, -0.7176],\n",
       "          [-0.9843, -0.9922, -1.0000,  ..., -0.6549, -0.7490, -0.6314],\n",
       "          ...,\n",
       "          [-0.8431, -0.8588, -0.8980,  ..., -0.5765, -0.5608, -0.5529],\n",
       "          [-0.8588, -0.8902, -0.9137,  ..., -0.2078, -0.2549, -0.2706],\n",
       "          [-0.8824, -0.9059, -0.9294,  ..., -0.2627, -0.1922, -0.1216]]]]), 'labels': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_example(ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3fb1968-add4-4bc6-8a2b-d63b52266c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset beans (/Users/brinkley97/.cache/huggingface/datasets/beans/default/0.0.0/90c755fb6db1c0ccdad02e897a37969dbf070bed3755d4391e269ff70642d791)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59860c8091134bf484e0fc375b870540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset('beans')\n",
    "\n",
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1474a99-2651-4770-9be4-216736fb33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8de6b24f-89f2-44b3-b935-08f6c4355041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.5686, -0.5686, -0.5608,  ..., -0.0275,  0.1922, -0.2549],\n",
       "          [-0.6078, -0.6000, -0.5843,  ..., -0.0353, -0.0196, -0.2706],\n",
       "          [-0.6314, -0.6314, -0.6157,  ..., -0.2392, -0.3647, -0.2314],\n",
       "          ...,\n",
       "          [-0.5373, -0.5529, -0.5765,  ..., -0.0745, -0.0431, -0.0980],\n",
       "          [-0.5608, -0.5765, -0.5843,  ...,  0.3176,  0.1608,  0.1294],\n",
       "          [-0.5843, -0.5922, -0.6078,  ...,  0.2784,  0.1451,  0.2000]],\n",
       "\n",
       "         [[-0.7098, -0.7098, -0.7490,  ..., -0.3725, -0.1608, -0.6000],\n",
       "          [-0.7333, -0.7333, -0.7569,  ..., -0.3569, -0.3176, -0.5608],\n",
       "          [-0.7490, -0.7490, -0.7647,  ..., -0.5373, -0.6627, -0.5373],\n",
       "          ...,\n",
       "          [-0.7725, -0.7882, -0.8196,  ..., -0.2314, -0.0353,  0.0824],\n",
       "          [-0.7961, -0.8118, -0.8118,  ...,  0.1843,  0.3176,  0.3725],\n",
       "          [-0.8196, -0.8196, -0.8275,  ...,  0.0745,  0.2863,  0.3961]],\n",
       "\n",
       "         [[-0.9922, -0.9922, -1.0000,  ..., -0.5451, -0.3647, -0.7333],\n",
       "          [-0.9922, -0.9922, -1.0000,  ..., -0.5686, -0.5451, -0.7176],\n",
       "          [-0.9843, -0.9922, -1.0000,  ..., -0.6549, -0.7490, -0.6314],\n",
       "          ...,\n",
       "          [-0.8431, -0.8588, -0.8980,  ..., -0.5765, -0.5608, -0.5529],\n",
       "          [-0.8588, -0.8902, -0.9137,  ..., -0.2078, -0.2549, -0.2706],\n",
       "          [-0.8824, -0.9059, -0.9294,  ..., -0.2627, -0.1922, -0.1216]]],\n",
       "\n",
       "\n",
       "        [[[-0.5137, -0.4824, -0.4118,  ..., -0.0353, -0.0039, -0.2078],\n",
       "          [-0.4902, -0.4588, -0.4196,  ..., -0.0745, -0.0039, -0.2314],\n",
       "          [-0.4667, -0.4902, -0.5137,  ..., -0.0824, -0.0039, -0.2941],\n",
       "          ...,\n",
       "          [-0.4980, -0.4980, -0.5294,  ..., -0.2000, -0.2157, -0.3882],\n",
       "          [-0.5529, -0.5294, -0.5137,  ..., -0.1922, -0.1922, -0.3882],\n",
       "          [-0.5294, -0.5451, -0.5451,  ..., -0.1294, -0.1529, -0.2706]],\n",
       "\n",
       "         [[-0.1843, -0.2000, -0.1608,  ...,  0.2157,  0.2000, -0.0902],\n",
       "          [-0.1608, -0.1686, -0.1529,  ...,  0.1922,  0.2235, -0.0902],\n",
       "          [-0.1529, -0.2000, -0.2392,  ...,  0.1765,  0.2549, -0.1294],\n",
       "          ...,\n",
       "          [-0.7725, -0.7569, -0.7569,  ..., -0.4196, -0.4588, -0.6471],\n",
       "          [-0.8039, -0.7804, -0.7647,  ..., -0.4196, -0.4510, -0.6627],\n",
       "          [-0.7647, -0.7961, -0.8039,  ..., -0.3725, -0.4196, -0.5451]],\n",
       "\n",
       "         [[-0.7412, -0.8588, -0.8510,  ..., -0.3255, -0.2627, -0.5608],\n",
       "          [-0.7725, -0.8431, -0.8196,  ..., -0.3490, -0.2706, -0.5765],\n",
       "          [-0.8196, -0.8667, -0.8510,  ..., -0.3725, -0.2314, -0.5451],\n",
       "          ...,\n",
       "          [-0.5216, -0.5137, -0.5294,  ..., -0.2471, -0.2549, -0.4510],\n",
       "          [-0.5529, -0.5373, -0.5216,  ..., -0.2157, -0.2157, -0.4431],\n",
       "          [-0.5294, -0.5529, -0.5608,  ..., -0.1608, -0.1843, -0.3333]]]]), 'labels': [0, 0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds['train'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f275565-e9e9-45a9-91e9-f1651eddeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de287106-5d1e-44e9-be12-b0b4b36bc323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_51372/311509507.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f476853f0f4d1bab0a61f27e2fb760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "071842be-1254-471d-be52-78fc09748c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108edcfefb8640b69a1696142a813bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3497e419caa343a09e794bd613ac533c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = ds['train'].features['labels'].names\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0119944-08e9-4c73-a041-488d83bbc64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base-beans\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  fp16=False,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8d317fd-4657-4e51-af21-8a039e749e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"validation\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17e67311-747e-4141-9541-ca16c5128221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brinkley97/opt/anaconda3/envs/emotion_rec_from_audiovisual/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1034\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 260\n",
      "  Number of trainable parameters = 85800963\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [260/260 26:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.237945</td>\n",
       "      <td>0.902256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.033344</td>\n",
       "      <td>0.992481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 133\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans/checkpoint-100\n",
      "Configuration saved in ./vit-base-beans/checkpoint-100/config.json\n",
      "Model weights saved in ./vit-base-beans/checkpoint-100/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-beans/checkpoint-100/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 133\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans/checkpoint-200\n",
      "Configuration saved in ./vit-base-beans/checkpoint-200/config.json\n",
      "Model weights saved in ./vit-base-beans/checkpoint-200/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-beans/checkpoint-200/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./vit-base-beans/checkpoint-200 (score: 0.03334350138902664).\n",
      "Saving model checkpoint to ./vit-base-beans\n",
      "Configuration saved in ./vit-base-beans/config.json\n",
      "Model weights saved in ./vit-base-beans/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-beans/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         4.0\n",
      "  total_flos               = 298497957GF\n",
      "  train_loss               =      0.1373\n",
      "  train_runtime            =  0:26:27.64\n",
      "  train_samples_per_second =       2.605\n",
      "  train_steps_per_second   =       0.164\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13d31d8a-0be6-43c2-b688-8594ed1d907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 133\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.9925\n",
      "  eval_loss               =     0.0333\n",
      "  eval_runtime            = 0:00:17.75\n",
      "  eval_samples_per_second =      7.493\n",
      "  eval_steps_per_second   =      0.958\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prepared_ds['validation'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f44825d-38c5-40b5-8af1-8fba30e324b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
