{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73285297-aaac-4a84-9674-1e34fd5954c6",
   "metadata": {},
   "source": [
    "# Extracting Features with ViT\n",
    "- https://huggingface.co/docs/transformers/model_doc/vit#vision-transformer-vit\n",
    "- https://arxiv.org/abs/2010.11929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dec1477-5216-45f8-8bd2-a178186168bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import ViTImageProcessor, ViTFeatureExtractor, ViTModel\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba20bd1-54fe-4b63-af56-df2ee86d7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/Users/brinkley97/Documents/development/'\n",
    "CLASS_PATH = 'classes/csci_535_multimodal_probabilistic_learning/'\n",
    "DATASET_PATH = 'datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4fdf4ea-1350-468c-b1a4-1bcb9379d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f283bc-790a-4554-94f9-ce6722caff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faces_in_specific_folder_path\n",
    "# all_faces_in_specific_folder\n",
    "def extract_features(faces_in_specific_folder_path, faces_file_names):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    faces_in_specific_folder_path -- py str (of all faces in a specific folder)\n",
    "    faces_file_names -- py list (of all the file names in a specific folder)\n",
    "    \n",
    "    Return:\n",
    "    extracted_features -- py\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_features_per_video = torch.empty((0, 197, 768))\n",
    "        \n",
    "    for faces_file_names_idx in range(len(faces_file_names)):\n",
    "        path_to_specific_face = faces_in_specific_folder_path + faces_file_names[faces_file_names_idx]\n",
    "        \n",
    "        '''\n",
    "        Start ViT\n",
    "        '''\n",
    "        specific_frame = Image.open(path_to_specific_face)\n",
    "        inputs = processor(images=specific_frame, return_tensors=\"pt\")\n",
    "        # print(inputs)\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # last_hidden_states == representation (1 and 2 with GradCam)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        extracted_features_per_video = torch.vstack((extracted_features_per_video, last_hidden_states))\n",
    "        \n",
    "        \n",
    "    return extracted_features_per_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bdea021-aaf4-4ba7-9ecf-cc018693ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_path(path_to_faces):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters:\n",
    "    path_to_faces -- str (of a single path to all saved cropped faces)\n",
    "    \n",
    "    Function calls: \n",
    "    extract_features\n",
    "    \n",
    "    Return\n",
    "    folder, features -- tuple (of the folder the extracted features are from and the extracted features)\n",
    "    \"\"\"\n",
    "    list_folders_with_faces_name = []\n",
    "    store_features_from_faces = []\n",
    "    \n",
    "    \n",
    "    count_number_files_in_dir = 0 \n",
    "    face_folder_files = os.listdir(path_to_faces)\n",
    "    \n",
    "    for face_folder_file_idx in range(len(face_folder_files)):\n",
    "        specific_face_folder = face_folder_files[face_folder_file_idx]\n",
    "        path_to_faces_in_specific_folder = path_to_faces + specific_face_folder + \"/\"\n",
    "        \n",
    "        folder_exists = os.path.isdir(path_to_faces_in_specific_folder)\n",
    "        \n",
    "        if folder_exists == True:\n",
    "            list_folders_with_faces_name.append(specific_face_folder)\n",
    "            all_faces_in_specific_folder = os.listdir(path_to_faces_in_specific_folder)\n",
    "            \n",
    "            features_extracted_from_specific_face_folder = extract_features(path_to_faces_in_specific_folder, all_faces_in_specific_folder)\n",
    "            store_features_from_faces.append(features_extracted_from_specific_face_folder)\n",
    "            \n",
    "            count_number_files_in_dir += 1\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return list_folders_with_faces_name, store_features_from_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f9b18-5f69-4863-a454-274d2015bcb4",
   "metadata": {},
   "source": [
    "# CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d9d591d-229e-48ef-a2a0-28532aa24a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_D_PATH = BASE + CLASS_PATH + DATASET_PATH + 'git_lfs/CREMA-D/sample_VideoFlash/all_faces/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdbca00d-63cd-43fc-b822-a4995e82efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_of_fe_crema_d, fe_crema_d = get_video_path(CREMA_D_PATH)\n",
    "# fe_crema_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c30c6b97-221e-416b-8c39-ec94ba1332aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1001_DFA_HAP_XX_frames_to_cropped_face',\n",
       " '1001_DFA_NEU_XX_frames_to_cropped_face',\n",
       " '1001_DFA_SAD_XX_frames_to_cropped_face',\n",
       " '1001_DFA_DIS_XX_frames_to_cropped_face',\n",
       " '1001_DFA_ANG_XX_frames_to_cropped_face',\n",
       " '1001_DFA_FEA_XX_frames_to_cropped_face']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_of_fe_crema_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab2b8b30-16f2-4ff8-a298-43b97f10d8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65, 197, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the folders of cropped faces to extract features from; #frames in single folder should match #features extracted\n",
    "# size of extracted features (all should be 196, 768) \n",
    "fe_crema_d[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff39356f-8149-43c3-8334-1b10594422a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_storage_for_file_and_extracted_features(folder_names, features_extracted):\n",
    "    '''\n",
    "    Parameters:\n",
    "    folder_names -- py list (of file names, so 1 file per face)\n",
    "    features_extracted -- py list (features from corresponding file)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    file_name_with_features_extracted_dict = {}\n",
    "\n",
    "    for folder_name_idx in range(len(folder_names)):\n",
    "        speficic_folder = folder_names[folder_name_idx]\n",
    "        file_name_with_features_extracted_dict[speficic_folder] = features_extracted[folder_name_idx]\n",
    "\n",
    "    return file_name_with_features_extracted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "099ac7e8-6d4a-4ed3-adf0-90ab1ce6f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_with_fe_crema_d = create_storage_for_file_and_extracted_features(folder_of_fe_crema_d, fe_crema_d)\n",
    "# file_with_fe_crema_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0a109ab-eb29-42bb-9b50-41db6f2fbf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary(dictionary_to_save, save_crema_d_dict_path):\n",
    "    '''\n",
    "    Parameters:\n",
    "    dictionary_to_save -- py dic (of folder_names with corresponding features_extracted)\n",
    "    \n",
    "    '''\n",
    "    save_location = save_dict_path + 'crema_d_extracted_features.csv'\n",
    "    # print(save_location)\n",
    "    \n",
    "    write_to_csv = csv.writer(open(save_location, \"w\"))\n",
    "\n",
    "    # loop over dictionary keys and values\n",
    "    for key, val in dictionary_to_save.items():\n",
    "\n",
    "        # write every key and value to file\n",
    "        write_to_csv.writerow([key, val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd7cdcd6-5361-4bbe-84f5-3b733bdf6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_crema_d_dict_path = BASE + CLASS_PATH + DATASET_PATH + 'git_lfs/CREMA-D/sample_VideoFlash/' \n",
    "# save_crema_d_dict_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be55d9d8-b65b-4486-8f73-be2445a82f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dictionary(file_with_fe_crema_d, save_crema_d_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4a70c-2da6-4b85-be6a-82829402b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe_np = get_video_path(sub_set_r_video_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f8682-d3b7-4bfd-97f2-cb53e304fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_frames_extracted = fe_np[0]\n",
    "# len(list_frames_extracted[1]), len(list_frames_extracted[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de045e-7ce4-4053-b3a7-4c708697ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_with_frames_name = fe_np[1]\n",
    "# folder_with_frames_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082cc56-900f-4908-a6de-5d7acb0d2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts = {}\n",
    "# # keys = range(4)... folder_with_frames_name\n",
    "# # values = [\"Hi\", \"I\", \"am\", \"John\"]... list_frames_extracted\n",
    "# for i in range(len(folder_with_frames_name)):\n",
    "#     print(folder_with_frames_name[i])\n",
    "#     folder_with_frames_name[i] = list_frames_extracted[i]\n",
    "# print(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb42e8d-c77f-4d87-8b24-427e3c398adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(fe_crema_d[4]['pixel_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f111ad-d6ba-4b61-8645-5904c503a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list_frames_extracted[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05292a00-164d-429e-aec7-ab770cb3bbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03b1c9-75b1-4ee4-aac1-18ed2c236931",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_crema_d[0][12].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617925c-ac35-4ef1-a3ba-183f8308066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 132, 197, 768 => 1,2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a66bb9-5475-4171-9c29-c835a3efd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names = fe_np[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598788a-55ae-4b7e-a697-197e6b68dd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name_with_frame_dict = {}\n",
    "for folder_name in range(len(folder_names)):\n",
    "    folder = folder_names[folder_name]\n",
    "    folder_name_with_frame_dict[folder] = fe_np[0]\n",
    "    \n",
    "# folder_name_with_frame_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bb00c-52fd-488b-a7cf-aac2a34c10a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_name_with_frame_dict['MSP-IMPROV-S01A-F02-R-FF01_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17963f-a679-4d7a-9212-dcdb20a190cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_name_with_frame_dict['MSP-IMPROV-S01A-F03-R-FF01_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a9f18-3046-4e79-b4d7-eb714ca6b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSP_DATASET_PATH = 'msp/videos/'\n",
    "# MSP_VIDEO_FILES = 'full_r_and_t_mspVideoPaths.csv'\n",
    "# MSP_video_file_paths = BASE + CLASS_PATH + DATASET_PATH + MSP_DATASET_PATH + MSP_VIDEO_FILES\n",
    "\n",
    "# r_and_t_video_files = BASE + CLASS_PATH + DATASET_PATH + MSP_DATASET_PATH + 'r_and_t_frames/'\n",
    "# print(r_and_t_video_files)\n",
    "# sub_set_r_video_files = r_and_t_video_files + 'sub_set_r_frames/'\n",
    "# print(sub_set_r_video_files)\n",
    "# sub_set_t_video_files = r_and_t_video_files + 'sub_set_t_frames/'\n",
    "# print(sub_set_t_video_files)\n",
    "# save_r_cropped_frames = BASE + CLASS_PATH + DATASET_PATH + MSP_DATASET_PATH + 'face_r_frames/'\n",
    "# r_and_t_video_files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
