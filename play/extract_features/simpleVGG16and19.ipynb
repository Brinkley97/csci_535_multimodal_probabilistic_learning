{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec68378-4381-44ff-a87a-621e4e603394",
   "metadata": {},
   "source": [
    "# How to Use The Pre-Trained VGG Model to Classify Objects in Photographs\n",
    "\n",
    "- VGG = [Oxford Visual Geometry Group](https://www.robots.ox.ac.uk/~vgg/)\n",
    "- https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b069a684-4618-434d-957d-69d01c2bbfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 12:53:12.333785: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# for load_img, img_to_array\n",
    "from keras import utils\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.applications.vgg19 import decode_predictions\n",
    "from keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7a9c93-f17f-4878-9fd8-3ed328f3623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/Users/brinkley97/Documents/development/'\n",
    "CLASS_PATH = 'classes/csci_535_multimodal_probabilistic_learning/'\n",
    "DATASET_PATH = 'datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b076892-591c-4d82-92d5-0b7555a9b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_photo_classifier(model, image):\n",
    "    \"\"\"\n",
    "    model -- keras (VGG16, VGG19)\n",
    "    image -- str (of image with path to it)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # load an image from file\n",
    "    image = utils.load_img(image, target_size=(224, 224))\n",
    "    \n",
    "    # convert the image pixels to a numpy array\n",
    "    image = utils.img_to_array(image)\n",
    "    \n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    \n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    # predict the probability across all output classes\n",
    "    yhat = model.predict(image)\n",
    "    \n",
    "    # convert the probabilities to class labels\n",
    "    label = decode_predictions(yhat)\n",
    "    \n",
    "    # retrieve the most likely result, e.g. highest probability\n",
    "    # label = label[0][0]\n",
    "    \n",
    "    # print the classification\n",
    "    # print('%s (%.2f%%)' % (label[1], label[2]*100))\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373d61b5-28a2-4dbf-8632-684e4d9b8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = BASE + CLASS_PATH + DATASET_PATH + 'misc/mug.jpg'\n",
    "face = BASE + CLASS_PATH + DATASET_PATH + 'project/crema_d/faces_to_specific_emotion_folder_crema_d/dis_crema_d/1001_DFA_DIS_XX_frame1.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0074651-3067-4d1e-af7c-5a2881e4d72a",
   "metadata": {},
   "source": [
    "# Load Models\n",
    "- VGG16\n",
    "- VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e5593-ae99-4291-a416-f2fb64825eb7",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a7d8b1e-744d-47ea-b99f-ceef4851df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 12:53:17.614647: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "vgg16_model = VGG16()\n",
    "# print(vgg16_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1af471-f96f-4f0a-ae81-8ddb730069ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 251ms/step\n",
      "[[('n03063599', 'coffee_mug', 0.7010682), ('n03063689', 'coffeepot', 0.10507006), ('n07930864', 'cup', 0.07677946), ('n04398044', 'teapot', 0.039143745), ('n03950228', 'pitcher', 0.030508274)]]\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "[[('n04039381', 'racket', 0.21308136), ('n03595614', 'jersey', 0.19744936), ('n03942813', 'ping-pong_ball', 0.13060461), ('n09835506', 'ballplayer', 0.07497317), ('n04409515', 'tennis_ball', 0.07146441)]]\n"
     ]
    }
   ],
   "source": [
    "simple_photo_classifier(vgg16_model, image)\n",
    "simple_photo_classifier(vgg16_model, face)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22754b4f-9890-453b-829c-a8e422737fbd",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e2c3f9-45e5-44c0-b30e-0b954148f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "vgg19_model = VGG19()\n",
    "# print(vgg19_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a5a6ab-cb62-43b0-9e15-0b3baaf4252c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 225ms/step\n",
      "[[('n03063599', 'coffee_mug', 0.85456306), ('n07930864', 'cup', 0.062048204), ('n03063689', 'coffeepot', 0.03351127), ('n03950228', 'pitcher', 0.017441066), ('n04398044', 'teapot', 0.0118974615)]]\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "[[('n04039381', 'racket', 0.16634709), ('n03595614', 'jersey', 0.14727265), ('n09835506', 'ballplayer', 0.08886752), ('n03942813', 'ping-pong_ball', 0.080129914), ('n04409515', 'tennis_ball', 0.07990491)]]\n"
     ]
    }
   ],
   "source": [
    "simple_photo_classifier(vgg19_model, image)\n",
    "simple_photo_classifier(vgg19_model, face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9035a-8b76-455e-9f79-496a2c969639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a62c2-c11e-4836-adc7-74b74ae5e74c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
