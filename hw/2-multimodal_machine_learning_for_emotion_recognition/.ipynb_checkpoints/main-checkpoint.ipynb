{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aef6b48-080c-4d36-8c42-bb4f3d6a9c30",
   "metadata": {},
   "source": [
    "# HW 2 Multimodal Machine Learning for Emotion Recognition\n",
    "\n",
    "- main (this notebook) with sub notebooks\n",
    "    1. audio (acoustic)\n",
    "    2. text (lexical)\n",
    "    3. visual\n",
    "    4. early fusion \n",
    "    5. late fusion\n",
    "    6. results\n",
    "- IEMOCAP (Interactive Emotional Dyadic Motion Capture) database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ed91f-14a4-4ca3-8ee8-f3546c5d67d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TODOs\n",
    "- Imports + Load Data\n",
    "- Preprocess Files\n",
    "    1. [x] Reduce the temporal dimension from Audio and Visual files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496762d-0e98-4562-b1b6-0ba40eb442aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports + Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b7f251-4c3d-4a0e-ae22-07d27bffb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ipympl as mpl # to show (image) plots\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm, naive_bayes, neighbors, pipeline, datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c7727a-20df-426a-8440-ba502c7575c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"/Users/brinkley97/Documents/development/\"\n",
    "CLASS_PATH = \"classes/csci_535_multimodal_probabilistic_learning/\"\n",
    "DATASET_PATH = \"datasets/hw_2\"\n",
    "FILE = \"/iemocapRelativeAddressForFiles.csv\"\n",
    "file_paths = BASE + CLASS_PATH + DATASET_PATH + FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d995c9-e0a3-48fe-a337-a8db4cb64a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    original_data = pd.read_csv(file)\n",
    "    # original_data = pd.DataFrame(file)\n",
    "    copy_of_data = original_data.copy()\n",
    "    return copy_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7f56b7-2cf6-45b2-8b70-fea26b7e0702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name_list</th>\n",
       "      <th>speakers</th>\n",
       "      <th>visual_features</th>\n",
       "      <th>acoustic_features</th>\n",
       "      <th>lexical_features</th>\n",
       "      <th>emotion_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ses01F_impro01_F001</td>\n",
       "      <td>F01</td>\n",
       "      <td>/features/visual_features/Session1/Ses01F_impr...</td>\n",
       "      <td>/features/acoustic_features/Session1/Ses01F_im...</td>\n",
       "      <td>/features/lexical_features/Session1/Ses01F_imp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ses01F_impro01_M011</td>\n",
       "      <td>M01</td>\n",
       "      <td>/features/visual_features/Session1/Ses01F_impr...</td>\n",
       "      <td>/features/acoustic_features/Session1/Ses01F_im...</td>\n",
       "      <td>/features/lexical_features/Session1/Ses01F_imp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ses01F_impro02_F002</td>\n",
       "      <td>F01</td>\n",
       "      <td>/features/visual_features/Session1/Ses01F_impr...</td>\n",
       "      <td>/features/acoustic_features/Session1/Ses01F_im...</td>\n",
       "      <td>/features/lexical_features/Session1/Ses01F_imp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ses01F_impro02_F003</td>\n",
       "      <td>F01</td>\n",
       "      <td>/features/visual_features/Session1/Ses01F_impr...</td>\n",
       "      <td>/features/acoustic_features/Session1/Ses01F_im...</td>\n",
       "      <td>/features/lexical_features/Session1/Ses01F_imp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ses01F_impro02_F004</td>\n",
       "      <td>F01</td>\n",
       "      <td>/features/visual_features/Session1/Ses01F_impr...</td>\n",
       "      <td>/features/acoustic_features/Session1/Ses01F_im...</td>\n",
       "      <td>/features/lexical_features/Session1/Ses01F_imp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>Ses05M_script03_2_M029</td>\n",
       "      <td>M05</td>\n",
       "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
       "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
       "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>Ses05M_script03_2_M039</td>\n",
       "      <td>M05</td>\n",
       "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
       "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
       "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>Ses05M_script03_2_M041</td>\n",
       "      <td>M05</td>\n",
       "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
       "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
       "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>Ses05M_script03_2_M042</td>\n",
       "      <td>M05</td>\n",
       "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
       "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
       "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>Ses05M_script03_2_M043</td>\n",
       "      <td>M05</td>\n",
       "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
       "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
       "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1336 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_name_list speakers  \\\n",
       "0        Ses01F_impro01_F001      F01   \n",
       "1        Ses01F_impro01_M011      M01   \n",
       "2        Ses01F_impro02_F002      F01   \n",
       "3        Ses01F_impro02_F003      F01   \n",
       "4        Ses01F_impro02_F004      F01   \n",
       "...                      ...      ...   \n",
       "1331  Ses05M_script03_2_M029      M05   \n",
       "1332  Ses05M_script03_2_M039      M05   \n",
       "1333  Ses05M_script03_2_M041      M05   \n",
       "1334  Ses05M_script03_2_M042      M05   \n",
       "1335  Ses05M_script03_2_M043      M05   \n",
       "\n",
       "                                        visual_features  \\\n",
       "0     /features/visual_features/Session1/Ses01F_impr...   \n",
       "1     /features/visual_features/Session1/Ses01F_impr...   \n",
       "2     /features/visual_features/Session1/Ses01F_impr...   \n",
       "3     /features/visual_features/Session1/Ses01F_impr...   \n",
       "4     /features/visual_features/Session1/Ses01F_impr...   \n",
       "...                                                 ...   \n",
       "1331  /features/visual_features/Session5/Ses05M_scri...   \n",
       "1332  /features/visual_features/Session5/Ses05M_scri...   \n",
       "1333  /features/visual_features/Session5/Ses05M_scri...   \n",
       "1334  /features/visual_features/Session5/Ses05M_scri...   \n",
       "1335  /features/visual_features/Session5/Ses05M_scri...   \n",
       "\n",
       "                                      acoustic_features  \\\n",
       "0     /features/acoustic_features/Session1/Ses01F_im...   \n",
       "1     /features/acoustic_features/Session1/Ses01F_im...   \n",
       "2     /features/acoustic_features/Session1/Ses01F_im...   \n",
       "3     /features/acoustic_features/Session1/Ses01F_im...   \n",
       "4     /features/acoustic_features/Session1/Ses01F_im...   \n",
       "...                                                 ...   \n",
       "1331  /features/acoustic_features/Session5/Ses05M_sc...   \n",
       "1332  /features/acoustic_features/Session5/Ses05M_sc...   \n",
       "1333  /features/acoustic_features/Session5/Ses05M_sc...   \n",
       "1334  /features/acoustic_features/Session5/Ses05M_sc...   \n",
       "1335  /features/acoustic_features/Session5/Ses05M_sc...   \n",
       "\n",
       "                                       lexical_features  emotion_labels  \n",
       "0     /features/lexical_features/Session1/Ses01F_imp...               3  \n",
       "1     /features/lexical_features/Session1/Ses01F_imp...               0  \n",
       "2     /features/lexical_features/Session1/Ses01F_imp...               1  \n",
       "3     /features/lexical_features/Session1/Ses01F_imp...               3  \n",
       "4     /features/lexical_features/Session1/Ses01F_imp...               1  \n",
       "...                                                 ...             ...  \n",
       "1331  /features/lexical_features/Session5/Ses05M_scr...               0  \n",
       "1332  /features/lexical_features/Session5/Ses05M_scr...               0  \n",
       "1333  /features/lexical_features/Session5/Ses05M_scr...               0  \n",
       "1334  /features/lexical_features/Session5/Ses05M_scr...               0  \n",
       "1335  /features/lexical_features/Session5/Ses05M_scr...               0  \n",
       "\n",
       "[1336 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 classes - anger(0), sadness(1) and happiness(2),and neutral(3)\n",
    "dataset_paths_copy = load_data(file_paths)\n",
    "dataset_paths_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b418a-a88f-42f5-bcc3-b5f24b41dd67",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing Files\n",
    "\n",
    "- [x] Build paths to specific files\n",
    "- [x] Reduce the time (temporal) dimension to 1 for audio and visual features\n",
    "- [x] Split data into train, test, and validation sets\n",
    "- *NOTE:* I choose NOT to call the functions below but in their specific file -- `[specific feature]-main.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9666f957-8823-4533-9904-bace91e5bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_paths_to_file(df_with_paths, specific_feature):\n",
    "    \"\"\"With the given DataFrame of paths, build my paths to access the files\n",
    "    \n",
    "    Parameters:\n",
    "    df_with_paths -- pd DF of paths to each \".npy\" file\n",
    "    specific_feature -- str (either visual features, acoustic features, or lexical features)\n",
    "    \n",
    "    Return:\n",
    "    list_of_features -- list (the paths that belong to that specific feature)\n",
    "    features -- pd DF (of emotion labels for that specific feature)\n",
    "    \"\"\"\n",
    "    \n",
    "    features = df_with_paths.loc[0:, ['file_name_list', 'speakers', specific_feature, 'emotion_labels']]\n",
    "    features_path = features.loc[0:, specific_feature]\n",
    "    features[\"file_with_path\"] = BASE + CLASS_PATH + DATASET_PATH + features_path\n",
    "    list_of_features = list(features[\"file_with_path\"])\n",
    "    \n",
    "    return list_of_features, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5afb5d8d-fe02-46e0-8d78-2476373f2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific_feature = 'acoustic_features'\n",
    "# audio_features_paths, audio_features_with_y = build_paths_to_file(dataset_paths_copy, specific_feature)\n",
    "\n",
    "# specific_feature = 'lexical_features'\n",
    "# text_features_paths, text_features_with_y = build_paths_to_file(dataset_paths_copy, specific_feature)\n",
    "\n",
    "# specific_feature = 'visual_features'\n",
    "# visual_features_paths, visual_features_with_y = build_paths_to_file(dataset_paths_copy, specific_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f262847-086d-4fe2-b0d9-8f771c7143b5",
   "metadata": {},
   "source": [
    "## 1. Reduce Temporal Dimension for Both Audio and Visual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f312eb7-5eae-43cb-b61d-0c593bab4aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_temporal_dimension(features_paths, ys):\n",
    "    \"\"\"Reduce audio and visual features by making the time dimension 1\n",
    "    \n",
    "    Parameters:\n",
    "    features_paths -- list (of the paths that belong to that specific feature)\n",
    "    ys -- pd Series (of the emotion labels that belong to that specifuc feature)\n",
    "    \n",
    "    Return: \n",
    "    reduced_features -- list (of reduced shapes of each input)\n",
    "    \"\"\"\n",
    "    \n",
    "    reduced_features = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for row in range(len(features_paths)):\n",
    "        # print(\"Current path with files is: \", features_path)\n",
    "        path_exists = os.path.exists(features_paths[row])\n",
    "        # print(path_exists)\n",
    "        if path_exists == True:\n",
    "            # print(\"Current path with files is: \", audio_features_path)\n",
    "            load_features_file = np.load(features_paths[row])\n",
    "            # print(\"  Original Shape: \", np.shape(load_audio_features_file))\n",
    "            resampled = np.mean(load_features_file, axis=0)\n",
    "            # print(\"  Reduced shape: \", np.shape(resampled_audio))\n",
    "            reduced_features.append(resampled)\n",
    "            # print(\"  reduced_audio_features: \", np.shape(reduced_audio_features))\n",
    "            true_labels.append(ys[row])\n",
    "            # print()\n",
    "        else:\n",
    "            pass\n",
    "            # print(\"CANNOT find current path: \", audio_features_path)\n",
    "    return reduced_features, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9565e8af-d36d-4640-a469-5bbc6d9e97b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_features(text_features_paths, ys):\n",
    "    \"\"\"Reduce from 2D to 1D by removing the time dimension\n",
    "    \n",
    "    text_features_paths -- list\n",
    "    \n",
    "    Return \n",
    "    reduced shapes of each input -- list\n",
    "    \"\"\"\n",
    "    \n",
    "    loaded_text_features = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for row in range(len(text_features_paths)):\n",
    "        # print(\"Current path with files is: \", text_features_paths)\n",
    "        path_exists = os.path.exists(text_features_paths[row])\n",
    "        # print(path_exists)\n",
    "        if path_exists == True:\n",
    "            # print(\"Current path with files is: \", text_features_paths[row])\n",
    "            load_text_features_file = np.load(text_features_paths[row])\n",
    "            # print(\"  Original Shape: \", np.shape(load_text_features_file))\n",
    "            # resampled_text = np.mean(load_text_features_file, axis=0)\n",
    "            # print(\"  Reduced shape: \", np.shape(resampled_text))\n",
    "            loaded_text_features.append(load_text_features_file)\n",
    "            # print(\"  reduced_text_features: \", np.shape(reduced_text_features))\n",
    "            true_labels.append(ys[row])\n",
    "            # print()\n",
    "        else:\n",
    "            pass\n",
    "            # print(\"CANNOT find current path: \", audio_features_path)\n",
    "    return loaded_text_features, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cb00d05-605e-40d4-8031-320576695fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(specific_features, specific_features_true_labels, test_size):\n",
    "    \"\"\"Split data into TRAIN, TEST, and VALIDATION sets\n",
    "    \n",
    "    Parameters:\n",
    "    specific_features -- list (of the reduced features)\n",
    "    specific_features_true_labels -- list (of the emotion labels that belong to that specifuc feature)\n",
    "    test_size -- float (to pass into sklearn train_test_split())\n",
    "    \n",
    "    Return:\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val -- list (for that specific subset of the features)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(specific_features, specific_features_true_labels, test_size=test_size, random_state=42)\n",
    "    total_X = len(X_train) + len(X_test)\n",
    "    total_Y = len(y_train) + len(y_test)\n",
    "    \n",
    "    print(\"[INFO] X, y TRAIN sets\")\n",
    "    print(np.shape(X_train), np.shape(y_train))\n",
    "    \n",
    "    print(\"\\n[INFO] X, y TEST sets\")\n",
    "    print(np.shape(X_test), np.shape(y_test))\n",
    "    # print(\"[INFO] TOTAL TRAIN, TEST sets\")\n",
    "    # print(total_X, total_Y)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.7, random_state=42)\n",
    "    total_X = len(X_train) + len(X_val)\n",
    "    total_Y = len(y_train) + len(y_val)\n",
    "    \n",
    "    print(\"\\n[INFO] X, y TRAIN sets\")\n",
    "    print(np.shape(X_train), np.shape(y_train))\n",
    "    \n",
    "    print(\"\\n[INFO] X, y VALIDATION sets\")\n",
    "    print(np.shape(X_val), np.shape(y_val))\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a050f-6700-407d-acbd-30ecf2fdd4fe",
   "metadata": {},
   "source": [
    "## 2. 4-class Emotion Classification + 3. Classification Results on Each Modality\n",
    "\n",
    "Unimodals, so no fusion. This will tell us how much each modality is contributing to our next step, which is fusion. Within each individual notebook, you'll see two sections - (1) \"Without Hyper-Parameter Tuning\" and (2) \"With Hyper-Parameter Tuning\". \n",
    "\n",
    "- [x]  Audio: Initializing (1) and (2) with the LinearSVC() estimator. (2) performs slightly better WRT F1-micro on a 10-fold subject-independent cross validation. See audio-main.ipynb for more details. $ \\newline $\n",
    "\n",
    "- [x]  Text: Initializing (1) and (2) with both (a) naive_bayes.BernoulliNB() and (b) naive_bayes.GaussianNB() estimators to see which performs better WRT F1-micro on a 10-fold subject-independent cross validation. (b) performs better in both sections - (1) and (2). See text-main.ipynb for more details. $ \\newline $\n",
    "\n",
    "- [x]  Visual: Initializing (1) and (2) with the LinearSVC() estimator. (2) performs slightly better WRT F1-micro on a 10-fold subject-independent cross validation. See visual-main.ipynb for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d1f99-7a76-40b7-8612-98382f1b5f29",
   "metadata": {},
   "source": [
    "## 4. Class Imbalance\n",
    "\n",
    "Below, we can see how many classifications are made for each class. The ordering from least to greatest is (2) happiness, (1) sadness, (0) anger, and (3) neutral. Although differences occur with class imbalance, I haven't come across any problems when running anything above. I will include some methods to handle class imbalance in future work. I assume doing this will improve my F1-micro metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddae412a-168e-4bc4-b2dc-a507ec44b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_imbalance_check(df):\n",
    "    \"\"\"Differientiate which files belong to which emotion label\n",
    "    \n",
    "    Parameter:\n",
    "    df -- pandas DF with all the files and emotional labels\n",
    "    \n",
    "    Return;\n",
    "    dfs (4) -- pandas DFs with each DF being a classification and having their respective files\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    classes = len(df)\n",
    "    \n",
    "    for c in range(classes):\n",
    "        # print(c)\n",
    "        y0s = df.emotion_labels == 0\n",
    "        y0s_df = df[y0s]\n",
    "        \n",
    "        \n",
    "        y1s = df.emotion_labels == 1\n",
    "        y1s_df = df[y1s]\n",
    "        \n",
    "        y2s = df.emotion_labels == 2\n",
    "        y2s_df = df[y2s]\n",
    "        \n",
    "        y3s = df.emotion_labels == 3\n",
    "        y3s_df = df[y3s]\n",
    "        \n",
    "    return y0s_df, y1s_df, y2s_df, y3s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a7e66e-e42b-4262-ad9a-478bdfd0e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0s_df, y1s_df, y2s_df, y3s_df = class_imbalance_check(dataset_paths_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe6a73e3-e18f-4e56-9b83-78bf91c3277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 308, 180, 520)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y0s_df), len(y1s_df), len(y2s_df), len(y3s_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea71dc-5e97-44c5-897b-9a511027fdf1",
   "metadata": {},
   "source": [
    "## 5. Fusion Results\n",
    "\n",
    "- [x] Early Fusion: Within, you'll see two sections - (1) \"Without Hyper-Parameter Tuning\" and (2) \"With Hyper-Parameter Tuning\". Initializing both sections  with 3 estimators - (1) svm.LinearSVC(), (2) naive_bayes.BernoulliNB(), (3) naive_bayes.GaussianNB(). In both (1) and (2), the estimators' performances from best to worse is (1), (2), (3). (a) When comparing sections (1) vs (2) for estimator (1), section (2) performs slighly better. (b) When comparing sections (1) vs (2) for estimator (2), section (2) performs slighly better. (c) When comparing sections (1) vs (2) for estimator (3), section (1) performs better. Not sure how section (1) performs better for (c). See notebook for more details.\n",
    "- [x] Late Fusion: Taking unimodal predictions from tasks 2 and 3. I'm excluding naive_bayes.BernoulliNB() as naive_bayes.GaussianNB() performs better. Doing majority vote over the three modalities. I then split and get the F1-micro metric. See notebook for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af65e38-1509-49ba-a3f8-fd1c1c80c0de",
   "metadata": {},
   "source": [
    "# 6. Interpretation on my results\n",
    "\n",
    "- Seems like early fusion using the LinearSVC() estimator performs the best overall. Poor results all around. I think this may be due to mean pooling in task 1. Future work, I'll test other pooling methods. See result notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd11846-c541-4ade-b22c-c51e46067ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
