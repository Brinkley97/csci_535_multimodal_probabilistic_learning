{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aef6b48-080c-4d36-8c42-bb4f3d6a9c30",
   "metadata": {},
   "source": [
    "# HW 2 Multimodal Machine Learning for Emotion Recognition\n",
    "\n",
    "- main (this notebook) with sub notebooks\n",
    "    1. audio (acoustic)\n",
    "    2. text (lexical)\n",
    "    3. visual\n",
    "- IEMOCAP (Interactive Emotional Dyadic Motion Capture) database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83070d1a-6675-496c-ad56-2bbb58a612e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_main_notebook = 'visual-main.ipynb'\n",
    "audio_main_notebook = 'audio-main.ipynb'\n",
    "text_main_notebook = 'text-main.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630bb2db-313b-43d1-a0a7-9a84a0cf7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load visual_main_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa1d8a9-2795-404a-9838-3d08362ed37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run 'visual-main.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ed91f-14a4-4ca3-8ee8-f3546c5d67d6",
   "metadata": {},
   "source": [
    "# TODOs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496762d-0e98-4562-b1b6-0ba40eb442aa",
   "metadata": {},
   "source": [
    "#  Imports + Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fcbf4a-2978-4d3d-91d6-817f606446c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4b7f251-4c3d-4a0e-ae22-07d27bffb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all subnotebooks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import librosa, librosa.display\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# for audio - maybe store in specific notebook?\n",
    "# import skimage.measure\n",
    "\n",
    "\n",
    "# for text\n",
    "\n",
    "# for visual\n",
    "import ipympl as mpl # to show (image) plots\n",
    "from sklearn import svm, datasets # per GridSearchCV documentation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28c7727a-20df-426a-8440-ba502c7575c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"/Users/brinkley97/Documents/development/\"\n",
    "CLASS_PATH = \"classes/csci_535_multimodal_probabilistic_learning/\"\n",
    "DATASET_PATH = \"datasets/hw_2\"\n",
    "\n",
    "# SESSION_1 = \"Session1/\"\n",
    "# SESSION_2 = \"Session2\"\n",
    "# SESSION_3 = \"Session3/\"\n",
    "# SESSION_4 = \"Session4/\"\n",
    "# SESSION_5 = \"Session5/\"\n",
    "\n",
    "# SES_01F = \"Ses01F_impro01/\"\n",
    "\n",
    "FILE = \"/iemocapRelativeAddressForFiles.csv\"\n",
    "file_paths = BASE + CLASS_PATH + DATASET_PATH + FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6d995c9-e0a3-48fe-a337-a8db4cb64a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    original_data = pd.read_csv(file)\n",
    "    # original_data = pd.DataFrame(file)\n",
    "    copy_of_data = original_data.copy()\n",
    "    return copy_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d7f56b7-2cf6-45b2-8b70-fea26b7e0702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name_list</th>\n",
       "      <th>speakers</th>\n",
       "      <th>visual_features</th>\n",
       "      <th>acoustic_features</th>\n",
       "      <th>lexical_features</th>\n",
       "      <th>emotion_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ses01F_impro01_F001</td>\n",
       "      <td>F01</td>\n",
       "      <td>/features/visual_features/Session1/Ses01F_impr...</td>\n",
       "      <td>/features/acoustic_features/Session1/Ses01F_im...</td>\n",
       "      <td>/features/lexical_features/Session1/Ses01F_imp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ses01F_impro01_M011</td>\n",
       "      <td>M01</td>\n",
       "      <td>/features/visual_features/Session1/Ses01F_impr...</td>\n",
       "      <td>/features/acoustic_features/Session1/Ses01F_im...</td>\n",
       "      <td>/features/lexical_features/Session1/Ses01F_imp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ses01F_impro02_F002</td>\n",
       "      <td>F01</td>\n",
       "      <td>/features/visual_features/Session1/Ses01F_impr...</td>\n",
       "      <td>/features/acoustic_features/Session1/Ses01F_im...</td>\n",
       "      <td>/features/lexical_features/Session1/Ses01F_imp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ses01F_impro02_F003</td>\n",
       "      <td>F01</td>\n",
       "      <td>/features/visual_features/Session1/Ses01F_impr...</td>\n",
       "      <td>/features/acoustic_features/Session1/Ses01F_im...</td>\n",
       "      <td>/features/lexical_features/Session1/Ses01F_imp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ses01F_impro02_F004</td>\n",
       "      <td>F01</td>\n",
       "      <td>/features/visual_features/Session1/Ses01F_impr...</td>\n",
       "      <td>/features/acoustic_features/Session1/Ses01F_im...</td>\n",
       "      <td>/features/lexical_features/Session1/Ses01F_imp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>Ses05M_script03_2_M029</td>\n",
       "      <td>M05</td>\n",
       "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
       "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
       "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>Ses05M_script03_2_M039</td>\n",
       "      <td>M05</td>\n",
       "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
       "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
       "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>Ses05M_script03_2_M041</td>\n",
       "      <td>M05</td>\n",
       "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
       "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
       "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>Ses05M_script03_2_M042</td>\n",
       "      <td>M05</td>\n",
       "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
       "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
       "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>Ses05M_script03_2_M043</td>\n",
       "      <td>M05</td>\n",
       "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
       "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
       "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1336 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_name_list speakers  \\\n",
       "0        Ses01F_impro01_F001      F01   \n",
       "1        Ses01F_impro01_M011      M01   \n",
       "2        Ses01F_impro02_F002      F01   \n",
       "3        Ses01F_impro02_F003      F01   \n",
       "4        Ses01F_impro02_F004      F01   \n",
       "...                      ...      ...   \n",
       "1331  Ses05M_script03_2_M029      M05   \n",
       "1332  Ses05M_script03_2_M039      M05   \n",
       "1333  Ses05M_script03_2_M041      M05   \n",
       "1334  Ses05M_script03_2_M042      M05   \n",
       "1335  Ses05M_script03_2_M043      M05   \n",
       "\n",
       "                                        visual_features  \\\n",
       "0     /features/visual_features/Session1/Ses01F_impr...   \n",
       "1     /features/visual_features/Session1/Ses01F_impr...   \n",
       "2     /features/visual_features/Session1/Ses01F_impr...   \n",
       "3     /features/visual_features/Session1/Ses01F_impr...   \n",
       "4     /features/visual_features/Session1/Ses01F_impr...   \n",
       "...                                                 ...   \n",
       "1331  /features/visual_features/Session5/Ses05M_scri...   \n",
       "1332  /features/visual_features/Session5/Ses05M_scri...   \n",
       "1333  /features/visual_features/Session5/Ses05M_scri...   \n",
       "1334  /features/visual_features/Session5/Ses05M_scri...   \n",
       "1335  /features/visual_features/Session5/Ses05M_scri...   \n",
       "\n",
       "                                      acoustic_features  \\\n",
       "0     /features/acoustic_features/Session1/Ses01F_im...   \n",
       "1     /features/acoustic_features/Session1/Ses01F_im...   \n",
       "2     /features/acoustic_features/Session1/Ses01F_im...   \n",
       "3     /features/acoustic_features/Session1/Ses01F_im...   \n",
       "4     /features/acoustic_features/Session1/Ses01F_im...   \n",
       "...                                                 ...   \n",
       "1331  /features/acoustic_features/Session5/Ses05M_sc...   \n",
       "1332  /features/acoustic_features/Session5/Ses05M_sc...   \n",
       "1333  /features/acoustic_features/Session5/Ses05M_sc...   \n",
       "1334  /features/acoustic_features/Session5/Ses05M_sc...   \n",
       "1335  /features/acoustic_features/Session5/Ses05M_sc...   \n",
       "\n",
       "                                       lexical_features  emotion_labels  \n",
       "0     /features/lexical_features/Session1/Ses01F_imp...               3  \n",
       "1     /features/lexical_features/Session1/Ses01F_imp...               0  \n",
       "2     /features/lexical_features/Session1/Ses01F_imp...               1  \n",
       "3     /features/lexical_features/Session1/Ses01F_imp...               3  \n",
       "4     /features/lexical_features/Session1/Ses01F_imp...               1  \n",
       "...                                                 ...             ...  \n",
       "1331  /features/lexical_features/Session5/Ses05M_scr...               0  \n",
       "1332  /features/lexical_features/Session5/Ses05M_scr...               0  \n",
       "1333  /features/lexical_features/Session5/Ses05M_scr...               0  \n",
       "1334  /features/lexical_features/Session5/Ses05M_scr...               0  \n",
       "1335  /features/lexical_features/Session5/Ses05M_scr...               0  \n",
       "\n",
       "[1336 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 classes - anger(0), sadness(1) and happiness(2),and neutral(3)\n",
    "dataset_paths_copy = load_data(file_paths)\n",
    "dataset_paths_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b418a-a88f-42f5-bcc3-b5f24b41dd67",
   "metadata": {},
   "source": [
    "# Preprocessing Files\n",
    "\n",
    "- [x] Build paths to specific files\n",
    "- [ ] Reduce the time (temporal) dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9666f957-8823-4533-9904-bace91e5bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_paths_to_file(df_with_paths, specific_feature):\n",
    "    \"\"\"With the given DataFrame of paths, build my paths...\n",
    "    \n",
    "    Parameters:\n",
    "    df_with_paths -- \n",
    "    specific_feature -- str (either visual_features, acoustic_features, or lexical_features)\n",
    "    \n",
    "    Return:\n",
    "    list\n",
    "    \"\"\"\n",
    "    \n",
    "    features = df_with_paths.loc[0:, ['file_name_list', 'speakers', specific_feature, 'emotion_labels']]\n",
    "    features_path = features.loc[0:, specific_feature]\n",
    "    features[\"file_with_path\"] = BASE + CLASS_PATH + DATASET_PATH + features_path\n",
    "    list_of_features = list(features[\"file_with_path\"])\n",
    "    \n",
    "    return list_of_features, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5afb5d8d-fe02-46e0-8d78-2476373f2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_feature = 'acoustic_features'\n",
    "audio_features_paths, audio_features_with_y = build_paths_to_file(dataset_paths_copy, specific_feature)\n",
    "\n",
    "specific_feature = 'lexical_features'\n",
    "text_features_paths, text_features_with_y = build_paths_to_file(dataset_paths_copy, specific_feature)\n",
    "\n",
    "\n",
    "specific_feature = 'visual_features'\n",
    "visual_features_paths, visual_features_with_y = build_paths_to_file(dataset_paths_copy, specific_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7c0f0-bd4a-4cc7-ae58-50068ac9b7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c326259-376f-4b2f-bb0d-81cb37cda63a",
   "metadata": {},
   "source": [
    "## Load Audio (Acoustic) features\n",
    "- We have VGGish, a deep convolutional neural network pre-trained on audio spectrograms extracted from a large database of videos to recognize a large variety of audio event categories [3]. The 128-dimensional embeddings were generated by VGGish after dimensionality reduction with Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b46385e-2c88-46bd-b403-d94573a7ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_features_path = BASE + CLASS_PATH + DATASET_PATH + '/features/acoustic_features/' + SESSION_1 + SES_01F\n",
    "# # audio_features\n",
    "# female_s1 = audio_features_path + 'Ses01F_impro01_F001.npy'\n",
    "# male_s1 = audio_features_path + 'Ses01F_impro01_M011.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feafd7bb-f087-4256-87bb-696c545179ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# female_s1_load = np.load(female_s1)\n",
    "# male_s1_load = np.load(male_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb940e7-a9bc-4d91-bd82-54c415e318e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(audio_features_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4578af-7680-4b9f-9c25-179ca718fd6e",
   "metadata": {},
   "source": [
    "## Load Text (Lexical) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5253c507-358b-409d-9ba9-1372626296fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2563c90-3d8b-4548-9fd1-821baa2f3921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1336,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(text_features_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8b732-2a56-4140-89b9-a07b5e4d083d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee403c76-babd-447c-886c-04aa6538b6fd",
   "metadata": {},
   "source": [
    "## Load Visual features\n",
    "\n",
    "- For the visual features, we have face embeddings obtained from a ResNet model [4] pre-trained on ImageNet. We have $ T × 2048 $ matrix for each utterance, where $ T $ denotes the number of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408b6a6-c07f-4926-ade5-ac1245cb7992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008aea47-edca-46d6-8edd-93636d6b6408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a88c8c5-377d-436b-a964-963a2174d017",
   "metadata": {},
   "source": [
    "# 3. Classification Results on the Visual Modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94a33789-1915-4200-b836-d65d96118414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual_f1_micro\n",
    "# audio_f1_micro\n",
    "# text_f1_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d1f99-7a76-40b7-8612-98382f1b5f29",
   "metadata": {},
   "source": [
    "# 4. Class Imbalance\n",
    "\n",
    "- Visual: Below, we can see how many classifications are made for each input. The ordering from least to greatest is (2) happiness, (1) sadness, (0) anger, and (3) neutral. Although differences occur with this modality, I haven't come across any problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddae412a-168e-4bc4-b2dc-a507ec44b291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5a7e66e-e42b-4262-ad9a-478bdfd0e5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe6a73e3-e18f-4e56-9b83-78bf91c3277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 308, 180, 520)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40ea71dc-5e97-44c5-897b-9a511027fdf1",
   "metadata": {},
   "source": [
    "# 5. Fusion Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4cb36-cf10-4fa4-9a20-d54b0ef96475",
   "metadata": {},
   "source": [
    "## Early Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3dd80f-8fcc-42fa-bf19-e9f9980ac327",
   "metadata": {},
   "source": [
    "## Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a191b4-81ea-4947-b9cd-03195de3e232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0af65e38-1509-49ba-a3f8-fd1c1c80c0de",
   "metadata": {},
   "source": [
    "# 6. Interpretation on my results\n",
    "\n",
    "1. Unimodals\n",
    "    - visual: Results aren't well due to pooling method. Recall that I'm using mean pooling which is the easiest to get started. Future work would have to implement other pooling methods to compare all respectively.\n",
    "    - audio:\n",
    "    - text: \n",
    "2. Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd11846-c541-4ade-b22c-c51e46067ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
