{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cba5bc0-6146-4d9d-b358-4f9dba73e869",
   "metadata": {},
   "source": [
    "# Loading ViT Feature Extractor Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b216be6f-0f7e-44b4-acfc-23c888b118b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset('beans')\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b138d6c8-dec8-4f32-9d58-746f03dfd779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_file_path': '/Users/brinkley97/.cache/huggingface/datasets/downloads/extracted/d91219caa561dea80e07fdc1f4941612865d20fcd0329f9fbc65a77755e5d9c8/train/bean_rust/bean_rust_train.148.jpg',\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500>,\n",
       " 'labels': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = ds['train'][400]\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c134c6-f9d0-49ba-93d4-6f112032b11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PIL.JpegImagePlugin.JpegImageFile,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = ex['image']\n",
    "type(image), image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4027ca9f-f21b-4618-a435-198d06ab4895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 15:50:41.975802: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/brinkley97/opt/anaconda3/envs/emotion_rec_from_audiovisual/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e20f2d-dea7-4ee0-bc08-bb9fcb4424f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "454be1a8-cd6b-4076-be80-4f74b44d788c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.image_processing_utils.BatchFeature"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_np = feature_extractor(image)\n",
    "type(fe_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b9a91-68d2-437b-a06b-df5eb34cd0c3",
   "metadata": {},
   "source": [
    "`plot pixel of image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e19d9fa2-d9b8-4d70-a504-8df6c07056f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3, 224, 224),\n",
       " [array([[[ 0.7882353 ,  0.67058825,  0.70980394, ..., -0.19215685,\n",
       "           -0.12941176, -0.17647058],\n",
       "          [ 0.7019608 ,  0.6       ,  0.6862745 , ..., -0.2862745 ,\n",
       "           -0.1607843 , -0.1607843 ],\n",
       "          [ 0.48235297,  0.4039216 ,  0.4666667 , ..., -0.19215685,\n",
       "           -0.01960784,  0.02745104],\n",
       "          ...,\n",
       "          [ 0.3803922 ,  0.5294118 ,  0.48235297, ..., -0.827451  ,\n",
       "           -0.81960785, -0.8039216 ],\n",
       "          [ 0.09019613,  0.37254906,  0.3803922 , ..., -0.8666667 ,\n",
       "           -0.84313726, -0.8509804 ],\n",
       "          [-0.05098039,  0.27843142,  0.3176471 , ..., -0.85882354,\n",
       "           -0.827451  , -0.827451  ]],\n",
       "  \n",
       "         [[ 0.4901961 ,  0.34901965,  0.3803922 , ..., -0.60784316,\n",
       "           -0.5372549 , -0.58431375],\n",
       "          [ 0.35686278,  0.20000005,  0.3176471 , ..., -0.7176471 ,\n",
       "           -0.6       , -0.5921569 ],\n",
       "          [ 0.04313731, -0.09019607,  0.05098045, ..., -0.6392157 ,\n",
       "           -0.47450978, -0.42745095],\n",
       "          ...,\n",
       "          [-0.2235294 , -0.05098039, -0.09019607, ..., -0.9764706 ,\n",
       "           -0.9607843 , -0.9372549 ],\n",
       "          [-0.5058824 , -0.20784312, -0.19215685, ..., -0.99215686,\n",
       "           -0.99215686, -1.        ],\n",
       "          [-0.64705884, -0.30196077, -0.24705881, ..., -0.9843137 ,\n",
       "           -0.9764706 , -0.9843137 ]],\n",
       "  \n",
       "         [[ 0.43529415,  0.27058828,  0.30980396, ..., -0.7176471 ,\n",
       "           -0.6392157 , -0.6862745 ],\n",
       "          [ 0.23921573,  0.082353  ,  0.20784318, ..., -0.8117647 ,\n",
       "           -0.67058825, -0.654902  ],\n",
       "          [-0.12156862, -0.23921567, -0.08235294, ..., -0.7019608 ,\n",
       "           -0.5372549 , -0.4980392 ],\n",
       "          ...,\n",
       "          [-0.27843136, -0.12941176, -0.19999999, ..., -0.94509804,\n",
       "           -0.94509804, -0.94509804],\n",
       "          [-0.6       , -0.30196077, -0.31764704, ..., -0.9843137 ,\n",
       "           -0.9764706 , -0.99215686],\n",
       "          [-0.7647059 , -0.40392154, -0.38823527, ..., -0.9764706 ,\n",
       "           -0.96862745, -0.96862745]]], dtype=float32)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fe_np.values\n",
    "np.shape(fe_np['pixel_values']), fe_np['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51027275-428a-4a6b-9d70-57ed60a13535",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_pt = feature_extractor(image, return_tensors='pt')\n",
    "# fe_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4b545-5c2a-450d-bdb8-d2a68fd112d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    inputs = feature_extractor(example['image'], return_tensors='pt')\n",
    "    inputs['labels'] = example['labels']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8d806-2fe0-4662-a36f-fd7e3f564a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_example(ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb1968-add4-4bc6-8a2b-d63b52266c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('beans')\n",
    "\n",
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1474a99-2651-4770-9be4-216736fb33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6b24f-89f2-44b3-b935-08f6c4355041",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ds['train'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f275565-e9e9-45a9-91e9-f1651eddeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de287106-5d1e-44e9-be12-b0b4b36bc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071842be-1254-471d-be52-78fc09748c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = ds['train'].features['labels'].names\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0119944-08e9-4c73-a041-488d83bbc64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base-beans\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  fp16=False,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d317fd-4657-4e51-af21-8a039e749e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"validation\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e67311-747e-4141-9541-ca16c5128221",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d31d8a-0be6-43c2-b688-8594ed1d907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(prepared_ds['validation'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f44825d-38c5-40b5-8af1-8fba30e324b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
